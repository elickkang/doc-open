

***扩展分区***

- **使用逻辑卷LVM**

  

- **未使用逻辑卷LVM**

  - *已存在针对已使用挂载点[^1]*

    1. 关闭涉及应用

       ```shell
  ps aux 
       kill -9
       ```
  
    2. 备份/app目录为/app/bak
  
       ```shell
       mkdir /app_bak
       mv /app/* /app_bak/
       ```
  
    3. 格式化新磁盘
  
       ```shell
        mkfs.ext4 /dev/vda5
       ```
  
    4. 挂接磁盘
  
       ```shell
       mount /dev/vdb /app
       ```
  
    5. 恢复 /app目录
  
       ```shell
       mv /app_bak/* /app
       ```
  
       
  
  - *新增挂载点*
  
    1. 新建挂载点
  
       ```shell
       mkdir /app
       ```
  
    2. 挂载目录
  
       ```shell
       mount /dev/vdb /app
       ```
  
       

***磁盘管理***

- **查看本机磁盘**
1. ```shell
   [root@VM_0_4_centos ~]# lsblk -f
     NAME   FSTYPE  LABEL    UUID                                 MOUNTPOINT
     sr0    iso9660 config-2 2019-09-10-14-48-20-00               
     vda                                                          
     └─vda1 ext4             4b499d76-769a-40a0-93dc-4a31a59add28 /
     vdb    ext4             03f25d04-73d7-4fa1-ab31-36624ff99a78 /app
   ```
  
  2. ```shell
      [root@VM_0_4_centos ~]# blkid
     /dev/sr0: UUID="2019-10-07-01-59-52-00" LABEL="config-2" TYPE="iso9660" 
     /dev/vda1: UUID="4b499d76-769a-40a0-93dc-4a31a59add28" TYPE="ext4" 
     /dev/vdb: UUID="03f25d04-73d7-4fa1-ab31-36624ff99a78" TYPE="ext4" 
     
     ```

- **查看磁盘分隔槽类型**

  ```shell
  [root@WHSERVER24 ~]# parted /dev/sda print
  Model: AVAGO INSPUR (scsi)
  Disk /dev/sda: 8398GB
  Sector size (logical/physical): 512B/512B
  Partition Table: gpt
  
  Number  Start   End     Size    File system  Name  Flags
   1      1049kB  211MB   210MB   ext4
   2      211MB   8398GB  8398GB                     lvm
  
  ```

  1. 针对mbr分隔槽类型使用fdisk进行分区,可能会以磁柱作为分割最小单位
  2. 针对gpt分隔槽类型使用gdisk进行分区，以磁区为最小分割单位

- **磁盘新增分隔槽后，分区实时生效**

  ```shell
  partprobe -s
  ```

- **查看系统当前分区情况**

  ```shell
  cat /proc/partitions
  ```

- **格式化分区**

  ```shell
  mkfs.xfs [-b bsize] [-d parms] [-i parms] [-l parms] [-L label] [-f] \
                           [-r parms]装置名称
  ```

  ```
  mkfs.ext4 [-b size] [-L label] 装置名称
  ```

- **查看格式化情况**

  ```shell
  dumpe2fs /dev/vdb
  Filesystem volume name:   <none>
  Last mounted on:          /app
  Filesystem UUID:          03f25d04-73d7-4fa1-ab31-36624ff99a78
  Filesystem magic number:  0xEF53
  Filesystem revision #:    1 (dynamic)
  Filesystem features:      has_journal ext_attr resize_inode dir_index filetype needs_recovery extent 64bit flex_bg sparse_super large_file huge_file uninit_bg dir_nlink extra_isize
  Filesystem flags:         signed_directory_hash 
  Default mount options:    user_xattr acl
  Filesystem state:         clean
  Errors behavior:          Continue
  Filesystem OS type:       Linux
  Inode count:              32768000
  Block count:              131072000
  Reserved block count:     6553600
  Free blocks:              128963642
  Free inodes:              32767989
  First block:              0
  Block size:               4096
  Fragment size:            4096
  Group descriptor size:    64
  Reserved GDT blocks:      1024
  Blocks per group:         32768
  Fragments per group:      32768
  Inodes per group:         8192
  Inode blocks per group:   512
  Flex block group size:    16
  Filesystem created:       Wed Oct 23 11:48:08 2019
  Last mount time:          Wed Oct 23 11:48:22 2019
  Last write time:          Wed Oct 23 11:48:22 2019
  Mount count:              1
  Maximum mount count:      -1
  Last checked:             Wed Oct 23 11:48:08 2019
  Check interval:           0 (<none>)
  Lifetime writes:          133 MB
  Reserved blocks uid:      0 (user root)
  Reserved blocks gid:      0 (group root)
  First inode:              11
  Inode size:               256
  Required extra isize:     28
  Desired extra isize:      28
  Journal inode:            8
  First orphan inode:       14942724
  Default directory hash:   half_md4
  Directory Hash Seed:      0d28b83d-c599-453e-a635-e9bfbc866467
  Journal backup:           inode blocks
  Journal features:         journal_incompat_revoke journal_64bit
  Journal size:             128M
  Journal length:           32768
  Journal sequence:         0x00051581
  Journal start:            8096
  
  
  Group 0: (Blocks 0-32767) [ITABLE_ZEROED]
    Checksum 0x4217, unused inodes 8179
    Primary superblock at 0, Group descriptors at 1-63
    Reserved GDT blocks at 64-1087
    Block bitmap at 1088 (+1088), Inode bitmap at 1104 (+1104)
    Inode table at 1120-1631 (+1120)
    23454 free blocks, 8182 free inodes, 1 directories, 8179 unused inodes
    Free blocks: 9313-9316, 9318-32767
    Free inodes: 11-8192
  
  ```

- **检查及修复磁盘**

  ​	*磁盘检查及修复的时候，可能会对部分文件系统修改，所以执行命令的时候，主要先卸载文件系统，即在卸载状态下进行文件系统的修复*

  *通常情况下都是在系统出现极大问题的时候才会使用该命令*

  1. xfs_repair 修复xfs格式的文件系统
  2. fsck.ext4 检查修复ext4格式的文件系统

- **挂载**

  挂载点是目录，而这个目录是进入磁盘分割槽（也就是档案系统）的入口，挂载之前需要明确几件事：

  1. 单一档案系统不能同时挂载到不同的文件系统

  2. 单一目录 不能同时挂载多个文件系统

  3. ***目录挂载之前应该确保是空目录***

  4. ***挂载CDROM之后，必须要先卸载才能取出碟片***

  5. 重新挂载根目录，并加入rw和auto属性

     ```shell
     mount -0 remount,rw,auto /
     ```

- **挂载某个目录到另一个目录，而不是挂载整个文件系统到一个目录**

  ```shell
  范例：将/var这个目录暂时挂载到/data/var底下： 
  [root@study ~]# mkdir /data/var 
  [root@study ~]# mount --bind /var /data/var 
  [root@study ~]# ls -lid /var /data/var
  16777346 drwxr-xr-x. 22 root root 4096 Jun 15 23:43 /data/var
  16777346 drwxr-xr-x. 22 root root 4096 Jun 15 23:43 /var
  #内容完全一模一样啊！因为挂载目录的缘故！
  ```
  
- **卸载设备**

  ```shell
  [root@study ~]# umount /dev/vda4       <==用装置档名来卸载 
  [root@study ~]# umount /data/ext4      <==用挂载点来卸载 
  [root@study ~]# umount /data/cdrom     <==因为挂载点比较好记忆！
  [root@study ~]# umount /data/usb      
  [root@study ~]# umount /data/var       <==一定要用挂载点！因为装置有被其他方式挂载
  ```

  - *卸载某个设备之前需要先退出该目录*

    ```shell
    [root@study ~]# mount /dev/sr0 /data/cdrom 
    [root@study ~]# cd /data/cdrom 
    [root@study cdrom]# umount /data/cdrom
    umount: /data/cdrom: target is busy.
            (In some cases useful info about processes that use
             the device is found by lsof(8) or fuser(1))
    
    [root@study cdrom]# cd / 
    [root@study /]# umount /data/cdrom
    ```

- **新增设备**

  基本上，Linux核心2.6版以后，硬体档名已经都可以被系统自动的即时产生了，我们根本不需要手动建立装置档案。不过某些情况底下我们可能还是得要手动处理装置档案的

  ```shell
  
  [root@study ~]# mknod装置档名[bcp] [Major] [Minor] 
  选项与参数：
  装置种类：
     b ：设定装置名称成为一个周边储存设备档案，例如磁碟等；
     c ：设定装置名称成为一个周边输入设备档案，例如滑鼠/键盘等；
     p ：设定装置名称成为一个FIFO 档案；
  Major ：主要装置代码；
  Minor ：次要装置代码；
  
  范例：由上述的介绍我们知道/dev/vda10装置代码252, 10，请建立并查阅此装置 
  [root@study ~]# mknod /dev/vda10 b 252 10 
  [root@study ~]# ll /dev/ vda10 
  b rw-r--r--. 1 root root 252, 10 Jun 24 23:40 /dev/vda10
   #上面那个252与10是有意义的，不要随意设定啊！
  ```

- **修改uuid及labelName**

  - xfs文件系统修改UUID及LabelName并尝试挂载

  ```shell
  [root@study ~]# xfs_admin [-lu] [-L label] [-U uuid]装置档名
  选项与参数：
  -l ：列出这个装置的label name
  -u ：列出这个装置的UUID
  -L ：设定这个装置的Label name
  -U ：设定这个装置的UUID 喔！
  
  范例：设定/dev/vda4的label name为vbird_xfs，并测试挂载 
  [root@study ~]# xfs_admin -L vbird_xfs /dev/vda4
  writing all SBs
  new label = "vbird_xfs"                  #产生新的LABEL名称啰！
  [root@study ~]# xfs_admin -l /dev/vda4
  label = "vbird_xfs"
  [root@study ~]# mount LABEL=vbird_xfs /data/xfs/
  
  范例：利用uuidgen产生新UUID来设定/dev/vda4，并测试挂载 
  [root@study ~]# umount /dev/vda4        #使用前，请先卸载！
  [root@study ~]# uuidgen 
  e0fa7252-b374-4a06-987a-3cb14f415488     #很有趣的指令！可以产生新的UUID喔！
  [root@study ~]# xfs_admin -u /dev/vda4
  UUID = e0a6af55-26e7-4cb7-a515-826a8bd29e90
  [root@study ~]# xfs_admin -U e0fa7252-b374-4a06-987a-3cb14f415488 /dev/vda4
  Clearing log and setting UUID
  writing all SBs
  new UUID = e0fa7252-b374-4a06-987a-3cb14f415488
  [root@study ~]# mount UUID=e0fa7252-b374-4a06-987a-3cb14f415488 /data/xfs
  ```

  - *ex4文件系统修改uuid及labelName并尝试挂载*

    ```shell
    
    [root@study ~]# tune2fs [-l] [-L Label] [-U uuid]装置档名
    选项与参数：
    -l ：类似dumpe2fs -h 的功能～将superblock 内的资料读出来～
    -L ：修改LABEL name
    -U ：修改UUID 啰！
    
    范例：列出/dev/vda5的label name之后，将它改成vbird_ext4 
    [root@study ~]# dumpe2fs -h /dev/vda5 | grep name
    dumpe2fs 1.42.9 (28-Dec-2013)
    Filesystem volume name: <none>    #果然是没有设定的！
    
    [root@study ~]# tune2fs -L vbird_ext4 /dev/vda5 
    [root@study ~]# dumpe2fs -h /dev/vda5 | grep name
    Filesystem volume name: vbird_ext4
    [root@study ~]# mount LABEL=vbird_ext4 /data/ext4
    ```

- **开机自动挂载**

  默认情况下,文件系统并没有开机挂载到目录树，所以需要设置开机自动挂载

  ```shell
  
  [root@study ~]# nano /etc/fstab 
  UUID="e0fa7252-b374-4a06-987a-3cb14f415488" /data/xfs xfs defaults 0 0
  ```

- **挂载iso镜像文件**

  iso镜像文件不需要烧录成CD或者制作成U盘然后去挂载，可以通过以下操作直接挂载。

  ```shell
  [root@study ~]# ll -h /tmp/CentOS-7.0-1406-x86_64-DVD.iso 
  -rw-r--r--. 1 root root 3.9G Jul 7 2014 /tmp/CentOS-7.0-1406 -x86_64-DVD.iso
   #看到上面的结果吧！这个档案就是映象档，档案非常的大吧！
  
  [root@study ~]# mkdir /data/centos_dvd 
  [root@study ~]# mount -o loop /tmp/CentOS-7.0-1406-x86_64-DVD.iso /data/centos_dvd 
  [root@study ~]# df / data/centos_dvd
  Filesystem 1K-blocks Used Available Use% Mounted on
  /dev/loop0 4050860 4050860 0 100% /data/centos_dvd
  # 就是这个项目！.iso 映象档内的所有资料可以在/data/centos_dvd 看到！
  
  [root@study ~]# ll /data/centos_dvd
  total 607
  -rw-r--r--. 1 500 502 14 Jul 5 2014 CentOS_BuildTag <==瞧！就是DVD的内容啊！
  drwxr-xr-x. 3 500 502 2048 Jul 4 2014 EFI
  -rw-r--r--. 1 500 502 611 Jul 5 2014 EULA
  -rw-r--r--. 1 500 502 18009 Jul 5 2014 GPL
  drwxr-xr-x. 3 500 502 2048 Jul 4 2014 images
  .....(底下省略).....
  
  [root@study ~]# umount /data/centos_dvd/ 
  #测试完成！记得将资料给他卸载！同时这个映像档也被鸟哥删除了...测试机容量不够大！
  ```

- **制作虚拟块设备，并完成挂载**

  1. *创建大文件，虚拟块设备*

     ```shell
     [root@study ~]# dd if=/dev/zero of=/srv/loopdev bs=1M count=512 
     512+0 records in    <==读入512笔资料 
     512+0 records out   <==输出512笔资料
     536870912 bytes (537 MB) copied, 12.3484 seconds, 43.5 MB/s
     # 这个指令的简单意义如下：
     # if 是input file ，输入档案。那个/dev/zero 是会一直输出0 的装置！
     # of 是output file ，将一堆零写入到后面接的档案中。
     # bs 是每个block 大小，就像档案系统那样的block 意义；
     # count 则是总共几个bs 的意思。所以bs*count 就是这个档案的容量了！
     
     [root@study ~]# ll -h /srv/loopdev
     -rw-r--r--. 1 root root 512M Jun 25 19:46 /srv/loopdev
     ```

  2. *格式化块设备*

     ```shell
     
     [root@study ~]# mkfs.xfs -f /srv/loopdev 
     [root@study ~]# blkid /srv/loopdev 
     /srv/loopdev: UUID="7dd97bd2-4446-48fd-9d23-a8b03ffdd5ee" TYPE="xfs "
     ```

  3. *挂载块设备*

     ```shell
     
     [root@study ~]# mount -o loop UUID="7dd97bd2-4446-48fd-9d23-a8b03ffdd5ee" /mnt 
     [root@study ~]# df /mnt
     Filesystem 1K-blocks Used Available Use% Mounted on
     /dev/loop0         520876 26372 494504 6% /mnt
     ```

- **创建SWAP分区**

- **磁盘限额**

  - *针对用户及群组设置限额*
  - *针对目录进行限额（目前xfs文件系统支持目录的限额，而ext只支持整个文件系统的限额）*

  针对群组的限额和针对目录的限额 不能同时设定，只能设定其一

- **磁盘阵列**

  ***Redundant Arrays of Independent Disks,RAID***
  
  ​	独立容错式磁盘阵列，RAID可以透过一个技术（软件或者硬件），将多个较小的磁盘整合成一个较大的磁盘装置，这个较大的磁盘不单单有存储功能，而且还具有资料保护的功能。整个RAID由于选择的等级不同，从而使得整合后的磁盘具有不同的功能，常见的LEVEL有：
  
  1. *RAID-0 (等量模式, stripe)：效能最佳*
  
     将数据分散到几个等量的磁盘中存储，提升了存取效率。但是如果一个磁盘损坏，则无法读取数据
  
     ![](..\..\IMG\raid0.gif)
  
  2. *RAID-1(映射模式,mirror)：完整备份*
  
     将完整的数据重复存放到不同的磁盘中，提升了数据安全性，但是降低了数据存取效率（如果使用专门的硬件处理磁盘读取性能尚可，但是如果使用软件处理，那么就会挤占系统CPU及IO总线带宽及DMA）
  
     ![](..\..\IMG\raid1.gif)
  
  3. *RAID 1+0，RAID 0+1*
  
     为了保障磁盘效能的同时关注磁盘数据安全性，中和以上两种RAID，实现1+0和0+1
  
     ![](..\..\IMG\raid1+0.jpg)
  
	   目前存储设备厂商推荐使用1+0，因为在保障高效性能的基础上可以最大程度保证数据安全
	
	5. *RAID 5*
	
	   ​	在RAID 0 的基础上增加了同位检查码，用于当磁盘损坏的时候可以重新修复磁盘，提升磁盘数据的安全性
	
	   ![](../../IMG/raid5.gif)
	
	   
	
	   ​	记录的同位检查码，每次都记录在不同的磁盘上，因此，任何一个磁盘损坏都能够借由其他磁盘的检查码来重建原来磁盘。由于有了同位检查码，因此RAID5的总容量会是整体磁盘数量减一。
	
	   ​	RAID 5 读取效能不错，不够由于每次都需要写入检查码，因此写效能不好。
	
	   ​	由于RAID5 仅支持一颗磁盘的损毁，因此近年来发展出另外一种等级，就是RAID 6，RAID使用两颗磁盘作为检查码的存储，因为整体磁盘容量就会少两颗，但是允许损坏的磁盘就是两颗。
	
	6. *Spare DIsk（预备磁盘的功能）*
	
	   ​	可以理解为备胎，在创建磁盘阵列的时候，留一个spare disk，然后如果磁盘阵列中有一个磁盘损毁，那么这颗磁盘会被主动的拉进磁盘阵列中，并将坏掉的那颗磁盘移除磁盘阵列，并重建资料系统
	
	7. *选择*
	
	   | 项目              | RAID0                      | RAID1      | RAID10             | RAID5      | RAID6       |
	   | ----------------- | -------------------------- | ---------- | ------------------ | ---------- | ----------- |
	   | 最少磁盘数        | 2                          | 2          | 4                  | 3          | 4           |
	   | 最大容错磁盘数(1) | 无                         | n-1        | n/2                | 1          | 2           |
	   | 资料安全性(1)     | 完全没有                   | 最佳       | 最佳               | 好         | 比 RAID5 好 |
	   | 理论写入效能(2)   | n                          | 1          | n/2                | <n-1       | <n-2        |
	   | 理论读出效能(2)   | n                          | n          | n                  | <n-1       | <n-2        |
	   | 可用容量(3)       | n                          | 1          | n/2                | n-1        | n-2         |
	   | 一般应用          | 强调效能但资料不重要的环境 | 资料与备份 | 服务器、云系统常用 | 资料与备份 | 资料与备份  |
	
	   *云环境下，既要确保资料安全同时又要能够足够快的对请求做出响应，因此效能比较差的RAID5、RAID6是不考虑的，RAID1+0可以考虑*
	
- **LVM逻辑卷轴管理**

  - *弹性调整文件系统的容量*
    1. PV(Physical Volume)，实体卷轴
    2. VG(Volume Group)，卷轴群组
    3. PE(Physical Extent)，实体范围区块
    4. LV(Logical Volume)，逻辑卷轴

  ![](../../IMG/pe_vg.gif)

  - *LVM操作流程*

    ![](../../IMG/centos7_lvm.jpg)

  - LVM具体流程

    - 磁盘分区
  
       MBR 分割表请使用 fdisk 分割， GPT 分割表请使用 gdisk 分割 
  
      ```shell
      [root@study ~]# gdisk /dev/vda
      Command (? for help): p
      Number  Start (sector)    End (sector)  Size       Code  Name
         1            2048            6143   2.0 MiB     EF02
         2            6144         2103295   1024.0 MiB  0700
         3         2103296        65026047   30.0 GiB    8E00
      # 找出最后一个 sector 的号码是很重要的！
      
      Command (? for help): ?  # 查一下增加分割的指令为何
      Command (? for help): n  # 就是这个！所以开始新增的行为！
      Partition number (4-128, default 4): 4  # 预设就是 4 号，所以也能 enter 即可！
      First sector (34-83886046, default = 65026048) or {+-}size{KMGTP}: 65026048  # 也能 enter
      Last sector (65026048-83886046, default = 83886046) or {+-}size{KMGTP}: +1G  # 决不要 enter
      # 这个地方可有趣了！我们不需要自己去计算磁区号码，透过 +容量 的这个方式，
      # 就可以让 gdisk 主动去帮你算出最接近你需要的容量的磁区号码喔！
      
      Current type is 'Linux filesystem'
      Hex code or GUID (L to show codes, Enter = 8300): # 使用预设值即可～直接 enter 下去！
      # 这裡在让你选择未来这个分割槽预计使用的档案系统！预设都是 Linux 档案系统的 8300 啰！
      
      Command (? for help): p
      Number  Start (sector)    End (sector)  Size       Code  Name
         1            2048            6143   2.0 MiB     EF02
         2            6144         2103295   1024.0 MiB  0700
         3         2103296        65026047   30.0 GiB    8E00
         4        65026048        67123199   1024.0 MiB  8300  Linux filesystem
      ```
  
    - 创建PV
  
      pvcreate
  
      ```shell
      # 1. 检查有无 PV 在系统上，然后将 /dev/vda{5-8} 建立成为 PV 格式
      [root@study ~]# pvscan
        PV /dev/vda3   VG centos   lvm2 [30.00 GiB / 14.00 GiB free]
        Total: 1 [30.00 GiB] / in use: 1 [30.00 GiB] / in no VG: 0 [0   ]
      # 其实安装的时候，我们就有使用 LVM 了喔！所以会有 /dev/vda3 存在的！
      
      [root@study ~]# pvcreate /dev/vda{5,6,7,8}
        Physical volume "/dev/vda5" successfully created
        Physical volume "/dev/vda6" successfully created
        Physical volume "/dev/vda7" successfully created
        Physical volume "/dev/vda8" successfully created
      # 这个指令可以一口气建立这四个 partition 成为 PV 啦！注意大括号的用途
      
      [root@study ~]# pvscan
        PV /dev/vda3   VG centos   lvm2 [30.00 GiB / 14.00 GiB free]
        PV /dev/vda8               lvm2 [1.00 GiB]
        PV /dev/vda5               lvm2 [1.00 GiB]
        PV /dev/vda7               lvm2 [1.00 GiB]
        PV /dev/vda6               lvm2 [1.00 GiB]
        Total: 5 [34.00 GiB] / in use: 1 [30.00 GiB] / in no VG: 4 [4.00 GiB]
      # 这就分别显示每个 PV 的资讯与系统所有 PV 的资讯。尤其最后一行，显示的是：
      # 整体 PV 的量 / 已经被使用到 VG 的 PV 量 / 剩余的 PV 量
      
      # 2. 更详细的列示出系统上面每个 PV 的个别资讯：
      [root@study ~]# pvdisplay /dev/vda5
        "/dev/vda5" is a new physical volume of "1.00 GiB"
        --- NEW Physical volume ---
        PV Name               /dev/vda5  <==实际的 partition 装置名称
        VG Name                          <==因为尚未分配出去，所以空白！
        PV Size               1.00 GiB   <==就是容量说明
        Allocatable           NO         <==是否已被分配，结果是 NO
        PE Size               0          <==在此 PV 内的 PE 大小
        Total PE              0          <==共分割出几个 PE
        Free PE               0          <==没被 LV 用掉的 PE
        Allocated PE          0          <==尚可分配出去的 PE 数量
        PV UUID               Cb717z-lShq-6WXf-ewEj-qg0W-MieW-oAZTR6
      # 由于 PE 是在建立 VG 时才给予的参数，因此在这裡看到的 PV 裡头的 PE 都会是 0
      # 而且也没有多余的 PE 可供分配 (allocatable)。
      ```
  
    - 创建VG
  
      vgcreate
  
      ```shell
      [root@study ~]# vgcreate [-s N[mgt]] VG名称 PV名称
      选项与参数：
      -s ：后面接 PE 的大小 (size) ，单位可以是 m, g, t (大小写均可)
      
      # 1. 将 /dev/vda5-7 建立成为一个 VG，且指定 PE 为 16MB 喔！
      [root@study ~]# vgcreate -s 16M vbirdvg /dev/vda{5,6,7}
        Volume group "vbirdvg" successfully created
      
      [root@study ~]# vgscan
        Reading all physical volumes.  This may take a while...
        Found volume group "vbirdvg" using metadata type lvm2  # 我们手动製作的
        Found volume group "centos" using metadata type lvm2   # 之前系统安装时作的
      
      [root@study ~]# pvscan
        PV /dev/vda5   VG vbirdvg   lvm2 [1008.00 MiB / 1008.00 MiB free]
        PV /dev/vda6   VG vbirdvg   lvm2 [1008.00 MiB / 1008.00 MiB free]
        PV /dev/vda7   VG vbirdvg   lvm2 [1008.00 MiB / 1008.00 MiB free]
        PV /dev/vda3   VG centos    lvm2 [30.00 GiB / 14.00 GiB free]
        PV /dev/vda8                lvm2 [1.00 GiB]
        Total: 5 [33.95 GiB] / in use: 4 [32.95 GiB] / in no VG: 1 [1.00 GiB]
      # 嘿嘿！发现没！有三个 PV 被用去，剩下 1 个 /dev/vda8 的 PV 没被用掉！
      
      [root@study ~]# vgdisplay vbirdvg
        --- Volume group ---
        VG Name               vbirdvg
        System ID
        Format                lvm2
        Metadata Areas        3
        Metadata Sequence No  1
        VG Access             read/write
        VG Status             resizable
        MAX LV                0
        Cur LV                0
        Open LV               0
        Max PV                0
        Cur PV                3
        Act PV                3
        VG Size               2.95 GiB        <==整体的 VG 容量有这么大
        PE Size               16.00 MiB       <==内部每个 PE 的大小
        Total PE              189             <==总共的 PE 数量共有这么多！
        Alloc PE / Size       0 / 0
        Free  PE / Size       189 / 2.95 GiB  <==尚可配置给 LV 的 PE数量/总容量有这么多！
        VG UUID               Rx7zdR-y2cY-HuIZ-Yd2s-odU8-AkTW-okk4Ea
      # 最后那三行指的就是 PE 能够使用的情况！由于尚未切出 LV，因此所有的 PE 均可自由使用。
      ```
  
    - 创建LV逻辑卷
  
      lvcreate
  
      ```shell
      [root@study ~]# lvcreate [-L N[mgt]] [-n LV名称] VG名称
      [root@study ~]# lvcreate [-l N] [-n LV名称] VG名称
      选项与参数：
      -L  ：后面接容量，容量的单位可以是 M,G,T 等，要注意的是，最小单位为 PE，
            因此这个数量必须要是 PE 的倍数，若不相符，系统会自行计算最相近的容量。
      -l  ：后面可以接 PE 的『个数』，而不是数量。若要这么做，得要自行计算 PE 数。
      -n  ：后面接的就是 LV 的名称啦！
      更多的说明应该可以自行查阅吧！ man lvcreate 
      
      # 1. 将 vbirdvg 分 2GB 给 vbirdlv 喔！
      [root@study ~]# lvcreate -L 2G -n vbirdlv vbirdvg
        Logical volume "vbirdlv" created
      # 由于本案例中每个 PE 为 16M ，如果要用 PE 的数量来处理的话，那使用下面的指令也 OK喔！
      # lvcreate -l 128 -n vbirdlv vbirdvg
      
      [root@study ~]# lvscan
        ACTIVE            '/dev/vbirdvg/vbirdlv' [2.00 GiB] inherit  <==新增加的一个 LV 啰！
        ACTIVE            '/dev/centos/root' [10.00 GiB] inherit
        ACTIVE            '/dev/centos/home' [5.00 GiB] inherit
        ACTIVE            '/dev/centos/swap' [1.00 GiB] inherit
      
      [root@study ~]# lvdisplay /dev/vbirdvg/vbirdlv
        --- Logical volume ---
        LV Path                /dev/vbirdvg/vbirdlv   # 这个是 LV 的全名喔！
        LV Name                vbirdlv
        VG Name                vbirdvg
        LV UUID                QJJrTC-66sm-878Y-o2DC-nN37-2nFR-0BwMmn
        LV Write Access        read/write
        LV Creation host, time study.centos.vbird, 2015-07-28 02:22:49 +0800
        LV Status              available
        # open                 0
        LV Size                2.00 GiB               # 容量就是这么大！
        Current LE             128
        Segments               3
        Allocation             inherit
        Read ahead sectors     auto
        - currently set to     8192
        Block device           253:3
      ```
  
    - 挂载LV
  
      mount
  
    - 放大LV
  
      lvresize
  
      ```shell
      # 1. 由前面的过程我们知道 /srv/lvm 是 /dev/vbirdvg/vbirdlv 这个装置，所以检查 vbirdvg 吧！
      [root@study ~]# vgdisplay vbirdvg
        --- Volume group ---
        VG Name               vbirdvg
        System ID
        Format                lvm2
        Metadata Areas        4
        Metadata Sequence No  3
        VG Access             read/write
        VG Status             resizable
        MAX LV                0
        Cur LV                1
        Open LV               1
        Max PV                0
        Cur PV                4
        Act PV                4
        VG Size               3.94 GiB
        PE Size               16.00 MiB
        Total PE              252
        Alloc PE / Size       128 / 2.00 GiB
        Free  PE / Size       124 / 1.94 GiB    # 看起来剩余容量确实超过 500M 的！
        VG UUID               Rx7zdR-y2cY-HuIZ-Yd2s-odU8-AkTW-okk4Ea
      # 既然 VG 的容量够大了！所以直接来放大 LV 吧！！
      
      # 2. 放大 LV 吧！利用 lvresize 的功能来增加！
      [root@study ~]# lvresize -L +500M /dev/vbirdvg/vbirdlv
        Rounding size to boundary between physical extents: 512.00 MiB
        Size of logical volume vbirdvg/vbirdlv changed from 2.00 GiB (128 extents) to 2.50 GiB 
      (160 extents).
        Logical volume vbirdlv successfully resized
      # 这样就增加了 LV 了喔！lvresize 的语法很简单，基本上同样透过 -l 或 -L 来增加！
      # 若要增加则使用 + ，若要减少则使用 - ！详细的选项请参考 man lvresize 啰！
      
      [root@study ~]# lvscan
        ACTIVE            '/dev/vbirdvg/vbirdlv' [2.50 GiB] inherit
        ACTIVE            '/dev/centos/root' [10.00 GiB] inherit
        ACTIVE            '/dev/centos/home' [5.00 GiB] inherit
        ACTIVE            '/dev/centos/swap' [1.00 GiB] inherit
      # 可以发现 /dev/vbirdvg/vbirdlv 容量由 2G 增加到 2.5G 啰！
      
      [root@study ~]# df -Th /srv/lvm
      Filesystem                  Type  Size  Used Avail Use% Mounted on
      /dev/mapper/vbirdvg-vbirdlv xfs   2.0G  111M  1.9G   6% /srv/lvm
      ```
  
    - 放大文件系统
  
      ```shell
      `# 3.1 先看一下原本的档案系统内的 superblock 记录情况吧！ [root@study ~]# xfs_info /srv/lvm meta-data=/dev/mapper/vbirdvg-vbirdlv isize=256    agcount=4, agsize=131072 blks         =                       sectsz=512   attr=2, projid32bit=1         =                       crc=0        finobt=0 data     =                       bsize=4096   blocks=524288, imaxpct=25         =                       sunit=0      swidth=0 blks naming   =version 2              bsize=4096   ascii-ci=0 ftype=0 log      =internal               bsize=4096   blocks=2560, version=2         =                       sectsz=512   sunit=0 blks, lazy-count=1 realtime =none                   extsz=4096   blocks=0, rtextents=0 [root@study ~]# xfs_growfs /srv/lvm  # 这一步骤才是最重要的！ [root@study ~]# xfs_info /srv/lvm meta-data=/dev/mapper/vbirdvg-vbirdlv isize=256    agcount=5, agsize=131072 blks         =                       sectsz=512   attr=2, projid32bit=1         =                       crc=0        finobt=0 data     =                       bsize=4096   blocks=655360, imaxpct=25         =                       sunit=0      swidth=0 blks naming   =version 2              bsize=4096   ascii-ci=0 ftype=0 log      =internal               bsize=4096   blocks=2560, version=2         =                       sectsz=512   sunit=0 blks, lazy-count=1 realtime =none                   extsz=4096   blocks=0, rtextents=0 [root@study ~]# df -Th /srv/lvm Filesystem                  Type  Size  Used Avail Use% Mounted on /dev/mapper/vbirdvg-vbirdlv xfs   2.5G  111M  2.4G   5% /srv/lvm [root@study ~]# ls -l /srv/lvm drwxr-xr-x. 131 root root 8192 Jul 28 00:12 etc drwxr-xr-x.  16 root root 4096 Jul 28 00:01 log # 刚刚复製进去的资料可还是存在的喔！并没有消失不见！`
      ```
  
  - LVM thin Pool*（LVM***自动动态***调整磁盘使用率）
  
  - *LVM 快照*（可用于磁盘备份与恢复）
[^1]: 已存在/app目录，目前需要拓展该目录的使用空间